# DAT108U_Data_Lake (Sparkify Data Lake ETL)
Udacity data lake with Apache Spark


This project demonstrates an ETL (Extract, Transform, Load) pipeline using Apache Spark for processing song and log data, and then storing the results in a data lake on Amazon S3.

## Project Structure

- `dl.cfg`: Configuration file containing AWS access keys.
- `etl.py`: Python script for ETL processing using Spark.

## Project Datasets

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata of the song and the artist.

The files are partitioned by the first three letters of each song's track ID. Below is the folder structure of the song data:

```
song_data/
│   ├── A/
│   │   ├── A/
|   |   |   |── A/
│   │   │   |   ├── TRAAAAW128F429D538.json
│   │   │   |   ├── TRAAABD128F429CF47.json
│   │   │   |   └── ...
│   │   |   ├── B/
│   │   ├── B/
```

### Log Dataset
The second dataset comprises JSON-format log files generated by an event simulator. These logs are created based on the songs from the previously mentioned dataset and mimic user activity in a music streaming application, following predefined configurations.

Within this dataset, the log files are organized and categorized by both the year and month of the logged events. Below is the folder structure for the log data:

```
log_data/
│   ├── 2018/
│   │   ├── 11/
|   |   |   |── 2018-11-01-events.json
│   │   │   ├── 2018-11-02-events.json
|   |   |   |...
```

## Prerequisites

Before running the ETL pipeline, make sure to set up your AWS access keys in `dl.cfg`.

## Getting Started

To get started, follow these steps:

1. Clone this repository to your local machine.

2. Set up your AWS access keys in the `dl.cfg` file:

   ```ini
   [IAM]
   AWS_ACCESS_KEY_ID = Your_AWS_Access_Key
   AWS_SECRET_ACCESS_KEY = Your_AWS_Secret_Key
   ```

3. Install the required dependencies, including Apache Spark.

4. Run the ETL pipeline by executing the `etl.py` script:

   ```bash
   python etl.py
   ```

   This will process song and log data, creating tables and storing the results in a specified S3 bucket.

## ETL Process

The ETL process is divided into two main functions:

- `process_song_data`: Processes song data and creates tables for songs and artists.
- `process_log_data`: Processes log data and creates tables for users, time, and songplays.

## Project Dependencies

This project depends on the following libraries:

- PySpark
- configparser
- datetime

You can install these dependencies using `pip`.


## Acknowledgments

- [Udacity](https://www.udacity.com) for providing the project template and data.

## Contact

For any questions or feedback, feel free to contact the project owner:
- Rash Afkhami
- Email: rash.afkhami@gmail.com
- GitHub: rash-afkh
